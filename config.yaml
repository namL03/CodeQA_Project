# Training Configuration for CodeQA Seq2seq Model
# Following "Get To The Point: Summarization with Pointer-Generator Networks" (See et al., 2017)

# =============================================================================
# Model Architecture (matching "Get To The Point" paper)
# =============================================================================
embed_dim: 128          # Word embedding dimension
hidden_dim: 256         # LSTM hidden state dimension
num_layers: 1           # Number of LSTM layers (single layer as per paper)
dropout: 0.0            # No dropout in base model
use_copy: true          # Enable pointer-generator (copy mechanism)

# =============================================================================
# Training Hyperparameters (matching "Get To The Point" paper)
# =============================================================================
batch_size: 16          # Batch size (adjust based on GPU memory)
epochs: 20              # Number of training epochs
learning_rate: 0.15     # Initial learning rate (Adagrad optimizer)
grad_clip: 2.0          # Gradient clipping threshold
teacher_forcing_ratio: 1.0  # Probability of using teacher forcing

# =============================================================================
# Data Parameters
# =============================================================================
max_src_len: 400        # Maximum source sequence length (question + code)
max_tgt_len: 50         # Maximum target sequence length (answer)
min_freq: 2             # Minimum frequency for vocabulary (already built)

# =============================================================================
# Training Settings
# =============================================================================
save_every: 1           # Save checkpoint every N epochs
num_workers: 4          # Number of data loading workers
device: cuda            # Device (cuda/cpu) - will auto-detect if cuda available

# =============================================================================
# Paths
# =============================================================================
data_dir: data
save_dir: saved_models
vocab_dir: saved_models

# =============================================================================
# Languages
# =============================================================================
# Choose one: python or java
language: python

# =============================================================================
# Notes:
# =============================================================================
# 1. This configuration matches the "Get To The Point" paper exactly
# 2. Adagrad optimizer is used (as per paper)
# 3. Learning rate starts at 0.15 (as per paper)
# 4. Single-layer bidirectional LSTM encoder
# 5. Single-layer unidirectional LSTM decoder
# 6. Bahdanau attention mechanism
# 7. Pointer-generator network (copy mechanism)
#
# To train with these settings:
# python scripts/train.py --language python
#
# To use custom settings:
# python scripts/train.py --language python --batch_size 32 --lr 0.1
