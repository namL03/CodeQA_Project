# Training Configuration for CodeQA Seq2seq Model
# Following "Get To The Point: Summarization with Pointer-Generator Networks" (See et al., 2017)

# =============================================================================
# Model Architecture (matching "Get To The Point" paper)
# =============================================================================
embed_dim: 128          # Word embedding dimension
hidden_dim: 256         # LSTM hidden state dimension
num_layers: 1           # Number of LSTM layers (single layer as per paper)
dropout: 0.0            # No dropout in base model
use_copy: true          # Enable pointer-generator (copy mechanism)

# =============================================================================
# Training Hyperparameters (matching "Get To The Point" paper)
# =============================================================================
batch_size: 16          # Batch size (adjust based on GPU memory)
epochs: 20              # Number of training epochs
learning_rate: 0.15     # Initial learning rate (Adagrad optimizer, accumulator=0.1)
grad_clip: 2.0          # Gradient clipping threshold (max gradient norm)
teacher_forcing_ratio: 1.0  # Probability of using teacher forcing
beam_size: 4            # Beam size for evaluation (as per paper)
early_stopping_patience: 5  # Early stopping: stop if no improvement for N epochs
compute_metrics_every: 0  # Compute BLEU/EM every N epochs (0 = only at end, for speed)

# =============================================================================
# Data Parameters (optimized based on data analysis)
# =============================================================================
max_src_len: 256        # Maximum source sequence length (covers 100% of examples)
max_tgt_len: 30         # Maximum target sequence length (covers 99.8% of examples)
min_freq: 2             # Minimum frequency for vocabulary (already built)

# =============================================================================
# Training Settings
# =============================================================================
save_every: 1           # Save checkpoint every N epochs
num_workers: 2          # Number of data loading workers
device: cuda            # Device (cuda/cpu) - will auto-detect if cuda available

# =============================================================================
# Paths
# =============================================================================
data_dir: data
save_dir: saved_models
vocab_dir: saved_models

# =============================================================================
# Languages
# =============================================================================
# Choose one: python or java
language: python

# =============================================================================
# Notes:
# =============================================================================
# 1. This configuration matches the "Get To The Point" paper exactly
# 2. Adagrad optimizer is used (as per paper)
# 3. Learning rate starts at 0.15 (as per paper)
# 4. Single-layer bidirectional LSTM encoder
# 5. Single-layer unidirectional LSTM decoder
# 6. Bahdanau attention mechanism
# 7. Pointer-generator network (copy mechanism)
#
# To train with these settings:
# python scripts/train.py --language python
#
# To use custom settings:
# python scripts/train.py --language python --batch_size 32 --lr 0.1
